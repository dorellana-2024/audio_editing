{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36214ceb",
   "metadata": {},
   "source": [
    "_______________________________________\n",
    "#### 1¬∞ Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Fix env windows\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"SPEECHBRAIN_CACHE_STRATEGY\"] = \"copy\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\",\n",
    "#     message=\".*deprecated.*\"\n",
    "# )\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face Token\n",
    "HF_TOKEN = os.getenv(\"PYANNOTE_HF_TOKEN\")\n",
    "assert HF_TOKEN, \"PYANNOTE_HF_TOKEN not defined.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65117ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../audios\"\n",
    "OUTPUT_DIR = \"../output\"\n",
    "PRE_CUT_SECONDS = 1.5  # segundos antes de que empiece el ni√±o\n",
    "MIN_CHILD_SEGMENT = 1.0  # duraci√≥n m√≠nima para considerar \"voz del ni√±o\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4f4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c189113dc664bd085986c099b607cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2933d8b555324172ab36a701384faa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d27ee5fb424a42843b975309e07bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c74adf7ae704d2e91739f3b8dfde824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "speaker-embedding.onnx:   0%|          | 0.00/26.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "   'pyannote/speaker-diarization-3.0',\n",
    "   use_auth_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# FUNCI√ìN PRINCIPAL\n",
    "# ==========================\n",
    "def process_audio(audio_path):\n",
    "    print(f\"Procesando: {os.path.basename(audio_path)}\")\n",
    "\n",
    "    # Cargar audio\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    duration = len(audio) / 1000  # segundos\n",
    "\n",
    "    # Diarizaci√≥n\n",
    "    diarization = pipeline(audio_path)\n",
    "\n",
    "    # Extraer segmentos ordenados\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"duration\": turn.end - turn.start\n",
    "        })\n",
    "\n",
    "    segments.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "    if len(segments) < 2:\n",
    "        print(\"‚ö†Ô∏è No se detectaron suficientes hablantes\")\n",
    "        return None\n",
    "\n",
    "    # ==========================\n",
    "    # ASUMIMOS:\n",
    "    # - Primer hablante = profesor\n",
    "    # - Segundo hablante largo = ni√±o\n",
    "    # ==========================\n",
    "    first_speaker = segments[0][\"speaker\"]\n",
    "\n",
    "    child_start = None\n",
    "    for seg in segments:\n",
    "        if seg[\"speaker\"] != first_speaker and seg[\"duration\"] >= MIN_CHILD_SEGMENT:\n",
    "            child_start = seg[\"start\"]\n",
    "            break\n",
    "\n",
    "    if child_start is None:\n",
    "        print(\"‚ö†Ô∏è No se detect√≥ inicio del ni√±o\")\n",
    "        return None\n",
    "\n",
    "    # Aplicar margen\n",
    "    cut_time = max(0, child_start - PRE_CUT_SECONDS)\n",
    "\n",
    "    # Cortar audio\n",
    "    cut_audio = audio[int(cut_time * 1000):]\n",
    "\n",
    "    return cut_audio, cut_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: TA10005.wav\n",
      "‚ö†Ô∏è No se detect√≥ inicio del ni√±o\n",
      "Procesando: TA10014.wav\n",
      "‚ö†Ô∏è No se detect√≥ inicio del ni√±o\n",
      "Procesando: TA10033.wav\n",
      "‚úî Guardado: ../output\\TA10033_cut.wav (corte en 9.05s)\n",
      "Procesando: TA40171.wav\n",
      "‚ö†Ô∏è No se detect√≥ inicio del ni√±o\n",
      "Procesando: TA40173.wav\n",
      "‚úî Guardado: ../output\\TA40173_cut.wav (corte en 5.42s)\n",
      "Procesando: TA40176.wav\n",
      "‚ö†Ô∏è No se detect√≥ inicio del ni√±o\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# PROCESAR CARPETA\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.lower().endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(INPUT_DIR, file)\n",
    "    output_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        file.replace(\".wav\", \"_cut.wav\")\n",
    "    )\n",
    "\n",
    "    result = process_audio(input_path)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    cut_audio, cut_time = result\n",
    "    cut_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"‚úî Guardado: {output_path} (corte en {cut_time:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e726cab",
   "metadata": {},
   "source": [
    "_________________\n",
    "##### 2¬∞ Prueba\n",
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57529abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\venv_py3.10\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Fix env windows\n",
    "# os.environ[\"SB_DISABLE_SYMLINKS\"] = \"1\"\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS\"] = \"1\"\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "# os.environ[\"SPEECHBRAIN_CACHE_STRATEGY\"] = \"copy\"\n",
    "\n",
    "PROJECT_CACHE = os.path.join(os.getcwd(), \".torch_cache\")\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = PROJECT_CACHE\n",
    "os.environ[\"SB_DISABLE_SYMLINKS\"] = \"1\"\n",
    "os.environ[\"SPEECHBRAIN_CACHE_STRATEGY\"] = \"copy\"\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "\n",
    "# Load environment variables from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Main order to garanty pyannote functuionality\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "from pydub.silence import detect_silence\n",
    "\n",
    "# Widgets\n",
    "# import ipywidgets as widgets\n",
    "# widgets.IntSlider()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\",\n",
    "#     message=\".*deprecated.*\"\n",
    "# )\n",
    "\n",
    "# Check token Hugging Face\n",
    "HF_TOKEN = os.getenv(\"PYANNOTE_HF_TOKEN\")\n",
    "assert HF_TOKEN, \"PYANNOTE_HF_TOKEN not defined.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442edc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIGURACI√ìN\n",
    "# =========================\n",
    "\n",
    "MIN_ACCUMULATED_CHILD_TIME = 1.5  # segundos acumulados del ni√±o\n",
    "PRE_CUT_SECONDS = 0.8             # margen antes del inicio del ni√±o\n",
    "MIN_SILENCE_LEN = 700             # ms\n",
    "SILENCE_THRESH = -40              # dBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77eb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# UTILIDADES\n",
    "# =========================\n",
    "\n",
    "def load_and_normalize_audio(path):\n",
    "    audio = AudioSegment.from_wav(path)\n",
    "    return normalize(audio)\n",
    "\n",
    "\n",
    "def diarize_audio(audio_path):\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization\",\n",
    "        use_auth_token=HF_TOKEN\n",
    "    )\n",
    "    return pipeline(audio_path)\n",
    "\n",
    "\n",
    "def diarization_to_segments(diarization):\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"duration\": turn.end - turn.start\n",
    "        })\n",
    "    return segments\n",
    "\n",
    "\n",
    "def detect_child_start_by_diarization(segments):\n",
    "    if not segments:\n",
    "        return None\n",
    "\n",
    "    first_speaker = segments[0][\"speaker\"]\n",
    "    acc_time = 0.0\n",
    "    start_time = None\n",
    "\n",
    "    for seg in segments:\n",
    "        if seg[\"speaker\"] != first_speaker:\n",
    "            if start_time is None:\n",
    "                start_time = seg[\"start\"]\n",
    "            acc_time += seg[\"duration\"]\n",
    "\n",
    "            if acc_time >= MIN_ACCUMULATED_CHILD_TIME:\n",
    "                return start_time\n",
    "        else:\n",
    "            acc_time = 0.0\n",
    "            start_time = None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def detect_child_start_by_silence(audio):\n",
    "    silences = detect_silence(\n",
    "        audio,\n",
    "        min_silence_len=MIN_SILENCE_LEN,\n",
    "        silence_thresh=SILENCE_THRESH\n",
    "    )\n",
    "\n",
    "    if not silences:\n",
    "        return None\n",
    "\n",
    "    # Tomamos el primer silencio largo\n",
    "    first_silence_end = silences[0][1]\n",
    "    return first_silence_end / 1000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FUNCI√ìN PRINCIPAL\n",
    "# =========================\n",
    "\n",
    "def extract_child_audio(input_wav, output_wav):\n",
    "    print(f\"\\nüéß Procesando: {os.path.basename(input_wav)}\")\n",
    "\n",
    "    audio = load_and_normalize_audio(input_wav)\n",
    "\n",
    "    # 1Ô∏è‚É£ Diarizaci√≥n\n",
    "    try:\n",
    "        diarization = diarize_audio(input_wav)\n",
    "        segments = diarization_to_segments(diarization)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en diarizaci√≥n: {e}\")\n",
    "        segments = []\n",
    "\n",
    "    # 2Ô∏è‚É£ Detecci√≥n por hablante\n",
    "    child_start = detect_child_start_by_diarization(segments)\n",
    "\n",
    "    # 3Ô∏è‚É£ Fallback por silencio\n",
    "    if child_start is None:\n",
    "        print(\"‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\")\n",
    "        child_start = detect_child_start_by_silence(audio)\n",
    "\n",
    "    if child_start is None:\n",
    "        print(\"‚ùå No se pudo detectar inicio del ni√±o\")\n",
    "        return False\n",
    "\n",
    "    cut_time = max(0, child_start - PRE_CUT_SECONDS)\n",
    "    print(f\"‚úÖ Inicio ni√±o detectado en {child_start:.2f}s (corte en {cut_time:.2f}s)\")\n",
    "\n",
    "    child_audio = audio[int(cut_time * 1000):]\n",
    "    child_audio.export(output_wav, format=\"wav\")\n",
    "\n",
    "    print(f\"üíæ Audio guardado: {output_wav}\")\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733832be",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"../audios\"\n",
    "OUTPUT_FOLDER = \"../output\"\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e9616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéß Procesando: TA10005.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daefd0ff8e154e5285d3b988c1839646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/500 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\venv_py3.10\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:43: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  backend = torchaudio.get_audio_backend()\n",
      "c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\venv_py3.10\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:45: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import (\n",
      "c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\venv_py3.10\\lib\\site-packages\\pyannote\\audio\\pipelines\\speaker_verification.py:53: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(backend)\n",
      "c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\venv_py3.10\\lib\\site-packages\\pyannote\\audio\\tasks\\segmentation\\mixins.py:37: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a8231fe4024a9da1c6930199c2a711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ffab05b8f24e748cac1d1627d708f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/318 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Denisse Orellana\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cpu. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\venv_py3.10\\lib\\site-packages\\speechbrain\\utils\\fetching.py:151: UserWarning: Using SYMLINK strategy on Windows for fetching potentially requires elevated privileges and is not recommended. See `LocalStrategy` documentation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error en diarizaci√≥n: [WinError 1314] El cliente no dispone de un privilegio requerido: 'c:\\\\Users\\\\Denisse Orellana\\\\Desktop\\\\editar_audio\\\\notebooks\\\\.hf_cache\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\0f99f2d0ebe89ac095bcc5903c4dd8f72b367286\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\Denisse Orellana\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'\n",
      "‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\n",
      "‚ùå No se pudo detectar inicio del ni√±o\n",
      "\n",
      "üéß Procesando: TA10014.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Denisse Orellana\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "‚ö†Ô∏è Error en diarizaci√≥n: [WinError 1314] El cliente no dispone de un privilegio requerido: 'c:\\\\Users\\\\Denisse Orellana\\\\Desktop\\\\editar_audio\\\\notebooks\\\\.hf_cache\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\0f99f2d0ebe89ac095bcc5903c4dd8f72b367286\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\Denisse Orellana\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'\n",
      "‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\n",
      "‚ùå No se pudo detectar inicio del ni√±o\n",
      "\n",
      "üéß Procesando: TA10033.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Denisse Orellana\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "‚ö†Ô∏è Error en diarizaci√≥n: [WinError 1314] El cliente no dispone de un privilegio requerido: 'c:\\\\Users\\\\Denisse Orellana\\\\Desktop\\\\editar_audio\\\\notebooks\\\\.hf_cache\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\0f99f2d0ebe89ac095bcc5903c4dd8f72b367286\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\Denisse Orellana\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'\n",
      "‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\n",
      "‚úÖ Inicio ni√±o detectado en 0.73s (corte en 0.00s)\n",
      "üíæ Audio guardado: ../output\\TA10033.wav\n",
      "\n",
      "üéß Procesando: TA40171.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Denisse Orellana\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "‚ö†Ô∏è Error en diarizaci√≥n: [WinError 1314] El cliente no dispone de un privilegio requerido: 'c:\\\\Users\\\\Denisse Orellana\\\\Desktop\\\\editar_audio\\\\notebooks\\\\.hf_cache\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\0f99f2d0ebe89ac095bcc5903c4dd8f72b367286\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\Denisse Orellana\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'\n",
      "‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\n",
      "‚ùå No se pudo detectar inicio del ni√±o\n",
      "\n",
      "üéß Procesando: TA40173.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Denisse Orellana\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "‚ö†Ô∏è Error en diarizaci√≥n: [WinError 1314] El cliente no dispone de un privilegio requerido: 'c:\\\\Users\\\\Denisse Orellana\\\\Desktop\\\\editar_audio\\\\notebooks\\\\.hf_cache\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\0f99f2d0ebe89ac095bcc5903c4dd8f72b367286\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\Denisse Orellana\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'\n",
      "‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\n",
      "‚úÖ Inicio ni√±o detectado en 0.99s (corte en 0.19s)\n",
      "üíæ Audio guardado: ../output\\TA40173.wav\n",
      "\n",
      "üéß Procesando: TA40176.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Denisse Orellana\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "‚ö†Ô∏è Error en diarizaci√≥n: [WinError 1314] El cliente no dispone de un privilegio requerido: 'c:\\\\Users\\\\Denisse Orellana\\\\Desktop\\\\editar_audio\\\\notebooks\\\\.hf_cache\\\\hub\\\\models--speechbrain--spkrec-ecapa-voxceleb\\\\snapshots\\\\0f99f2d0ebe89ac095bcc5903c4dd8f72b367286\\\\hyperparams.yaml' -> 'C:\\\\Users\\\\Denisse Orellana\\\\.cache\\\\torch\\\\pyannote\\\\speechbrain\\\\hyperparams.yaml'\n",
      "‚ö†Ô∏è Fallback: usando detecci√≥n por silencio\n",
      "‚ùå No se pudo detectar inicio del ni√±o\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(INPUT_FOLDER):\n",
    "    if file.lower().endswith(\".wav\"):\n",
    "        in_path = os.path.join(INPUT_FOLDER, file)\n",
    "        out_path = os.path.join(OUTPUT_FOLDER, file)\n",
    "        extract_child_audio(in_path, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df849ac3",
   "metadata": {},
   "source": [
    "___________________________________\n",
    "#### 3¬∞ Prueba silencios 12-01-2025\n",
    "Algoritmo para detectar silencios. No realiza diarizaci√≥n, ni segmentaci√≥n.\n",
    "* not working\n",
    "___________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440d247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "from pydub.silence import detect_silence\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_child_audio(\n",
    "    audio_path,\n",
    "    output_path,\n",
    "    min_silence_len=700,      # ms de silencio real\n",
    "    silence_thresh_offset=16, # dB bajo el promedio\n",
    "    min_reading_duration=3.0, # segundos m√≠nimos de lectura\n",
    "    pre_cut_seconds=0.8       # margen antes de que empiece el ni√±o\n",
    "):\n",
    "    \"\"\"\n",
    "    Detecta el inicio de la lectura del ni√±o usando:\n",
    "    - normalizaci√≥n\n",
    "    - detecci√≥n de silencio\n",
    "    - habla sostenida\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"üéß Procesando: {os.path.basename(audio_path)}\")\n",
    "\n",
    "    # =========================\n",
    "    # 1. Cargar y normalizar\n",
    "    # =========================\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    audio = normalize(audio)\n",
    "\n",
    "    duration_sec = len(audio) / 1000\n",
    "\n",
    "    # =========================\n",
    "    # 2. Detectar silencios\n",
    "    # =========================\n",
    "    silence_thresh = audio.dBFS - silence_thresh_offset\n",
    "\n",
    "    silences = detect_silence(\n",
    "        audio,\n",
    "        min_silence_len=min_silence_len,\n",
    "        silence_thresh=silence_thresh\n",
    "    )\n",
    "\n",
    "    if not silences:\n",
    "        print(\"‚ö†Ô∏è No se detectaron silencios claros\")\n",
    "        return False\n",
    "\n",
    "    # =========================\n",
    "    # 3. Buscar inicio del ni√±o\n",
    "    # =========================\n",
    "    child_start = None\n",
    "\n",
    "    for silence_start, silence_end in silences:\n",
    "        # El ni√±o deber√≠a empezar DESPU√âS del primer silencio largo\n",
    "        candidate_start = silence_end / 1000  # segundos\n",
    "\n",
    "        remaining = duration_sec - candidate_start\n",
    "        if remaining >= min_reading_duration:\n",
    "            child_start = candidate_start\n",
    "            break\n",
    "\n",
    "    if child_start is None:\n",
    "        print(\"‚ùå No se pudo detectar inicio del ni√±o\")\n",
    "        return False\n",
    "\n",
    "    # =========================\n",
    "    # 4. Aplicar margen\n",
    "    # =========================\n",
    "    cut_time = max(0, child_start - pre_cut_seconds)\n",
    "\n",
    "    child_audio = audio[int(cut_time * 1000):]\n",
    "\n",
    "    # =========================\n",
    "    # 5. Validar duraci√≥n final\n",
    "    # =========================\n",
    "    final_duration = len(child_audio) / 1000\n",
    "    if final_duration < min_reading_duration:\n",
    "        print(\"‚ùå Segmento demasiado corto para ser lectura\")\n",
    "        return False\n",
    "\n",
    "    # =========================\n",
    "    # 6. Exportar\n",
    "    # =========================\n",
    "    child_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"‚úÖ Ni√±o detectado desde {cut_time:.2f}s ‚Üí guardado\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83069988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéß Procesando: TA10005.wav\n",
      "‚ö†Ô∏è No se detectaron silencios claros\n",
      "üéß Procesando: TA10014.wav\n",
      "‚ö†Ô∏è No se detectaron silencios claros\n",
      "üéß Procesando: TA10033.wav\n",
      "‚úÖ Ni√±o detectado desde 0.03s ‚Üí guardado\n",
      "üéß Procesando: TA40171.wav\n",
      "‚ö†Ô∏è No se detectaron silencios claros\n",
      "üéß Procesando: TA40173.wav\n",
      "‚úÖ Ni√±o detectado desde 0.17s ‚Üí guardado\n",
      "üéß Procesando: TA40176.wav\n",
      "‚úÖ Ni√±o detectado desde 0.45s ‚Üí guardado\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER = \"../audios\"\n",
    "OUTPUT_FOLDER = \"../output\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(INPUT_FOLDER):\n",
    "    if file.lower().endswith(\".wav\"):\n",
    "        in_path = os.path.join(INPUT_FOLDER, file)\n",
    "        out_path = os.path.join(OUTPUT_FOLDER, file)\n",
    "\n",
    "        extract_child_audio(in_path, out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (venv_py3.10)",
   "language": "python",
   "name": "venv_py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
