{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32863cc",
   "metadata": {},
   "source": [
    "## __üéß Audio Diarization Project (pyannote.audio)__\n",
    "\n",
    "#### üß± Requisitos del sistema\n",
    "- Python 3.12.12\n",
    "- Cuenta en Hugging Face con acceso a modelos pyannote/\n",
    "\n",
    "## üìå Descripci√≥n\n",
    "Este proyecto utiliza el pipeline pyannote/speaker-diarization-3.0 para:\n",
    "- detectar segmentos de voz\n",
    "- identificar hablantes distintos\n",
    "- preparar el audio para cortes posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Check python version\n",
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Limpiar entorno previo\n",
    "!pip uninstall -y numpy torch torchaudio pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install ffmpeg\n",
    "!apt-get update\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d24cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Install main libraries\n",
    "!pip install \"numpy<2.0\" \\\n",
    "             \"torch==2.3.1\" \\\n",
    "             \"torchvision==0.18.1\" \\\n",
    "             \"torchaudio==2.3.1\" \\\n",
    "             \"onnxruntime\" \\\n",
    "             \"huggingface-hub>=0.19.4\" \\\n",
    "             \"pyannote.audio==3.1.1\" \\\n",
    "             \"pydub\" \\\n",
    "             \"ipywidgets\"\n",
    "\n",
    "# NOTA IMPORTANTE:\n",
    "# Al terminar, ve al men√∫ arriba: \"Entorno de ejecuci√≥n\" -> \"Reiniciar sesi√≥n\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "from pydub.silence import detect_silence\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HF token from Colab userdata\n",
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get('PYANNOTE_HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03893df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "   'pyannote/speaker-diarization-3.0',\n",
    "   use_auth_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571fa16",
   "metadata": {},
   "source": [
    "_______________\n",
    "1¬∞ Try: Diarization\n",
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea87f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"./audios\"\n",
    "OUTPUT_DIR = \"./output\"\n",
    "PRE_CUT_SECONDS = 1.5  # segundos antes de que empiece el ni√±o\n",
    "MIN_CHILD_SEGMENT = 1.0  # duraci√≥n m√≠nima para considerar \"voz del ni√±o\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cda714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# FUNCI√ìN PRINCIPAL\n",
    "# ==========================\n",
    "def process_audio(audio_path):\n",
    "    print(f\"Procesando: {os.path.basename(audio_path)}\")\n",
    "\n",
    "    # Cargar audio\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    duration = len(audio) / 1000  # segundos\n",
    "\n",
    "    # Diarizaci√≥n\n",
    "    diarization = pipeline(audio_path)\n",
    "\n",
    "    # Extraer segmentos ordenados\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"duration\": turn.end - turn.start\n",
    "        })\n",
    "\n",
    "    segments.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "    if len(segments) < 2:\n",
    "        print(\"‚ö†Ô∏è No se detectaron suficientes hablantes\")\n",
    "        return None\n",
    "\n",
    "    # ==========================\n",
    "    # Siempre se cumple:\n",
    "    # - Primer hablante = profesor\n",
    "    # - Segundo hablante largo = ni√±o\n",
    "    # ==========================\n",
    "    first_speaker = segments[0][\"speaker\"]\n",
    "\n",
    "    child_start = None\n",
    "    for seg in segments:\n",
    "        if seg[\"speaker\"] != first_speaker and seg[\"duration\"] >= MIN_CHILD_SEGMENT:\n",
    "            child_start = seg[\"start\"]\n",
    "            break\n",
    "\n",
    "    if child_start is None:\n",
    "        print(\"‚ö†Ô∏è No se detect√≥ inicio del ni√±o\")\n",
    "        return None\n",
    "\n",
    "    # Aplicar margen\n",
    "    cut_time = max(0, child_start - PRE_CUT_SECONDS)\n",
    "\n",
    "    # Cortar audio\n",
    "    cut_audio = audio[int(cut_time * 1000):]\n",
    "\n",
    "    return cut_audio, cut_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip audios.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101893aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# PROCESAR CARPETA\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.lower().endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(INPUT_DIR, file)\n",
    "    output_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        file.replace(\".wav\", \"_cut.wav\")\n",
    "    )\n",
    "\n",
    "    result = process_audio(input_path)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    cut_audio, cut_time = result\n",
    "    cut_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"‚úî Guardado: {output_path} (corte en {cut_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b898fe5",
   "metadata": {},
   "source": [
    "______________________\n",
    "2¬∞ Try: VAD\n",
    "‚ùå Problemas reales de diarizaci√≥n en estos audios\n",
    "\n",
    "El profesor y el ni√±o suelen ser clasificados como el mismo speaker\n",
    "Voces cercanas al micr√≥fono\n",
    "Duraci√≥n corta del profesor\n",
    "Ruido ambiente\n",
    "Voz infantil ‚Üí embeddings poco estables\n",
    "El ni√±o puede empezar a leer sin una pausa clara\n",
    "El modelo no detecta un ‚Äúcambio de hablante‚Äù\n",
    "Todo queda como SPEAKER_00\n",
    "Segmentos del ni√±o aparecen fragmentados\n",
    "Muchos segmentos cortos < 1.0s\n",
    "Nunca cumples duration >= MIN_CHILD_SEGMENT\n",
    "Diarizaci√≥n ‚â† detecci√≥n de ‚Äúinicio sem√°ntico‚Äù\n",
    "Se busca inicio de lectura\n",
    "El modelo solo sabe: qui√©n habla cu√°ndo\n",
    "Por lo tanto:\n",
    "\n",
    "No se busca detectar otro hablante.\n",
    "Se quiere identificar el momento en que comienza una voz continua distinta al c√≥digo inicial.\n",
    "Se utilizar√° VAD Voice activity detection -> ACTIVIDAD DE VOZ\n",
    "______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "vad = Pipeline.from_pretrained(\n",
    "    \"pyannote/voice-activity-detection\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "MIN_READING_DURATION = 4.0   # segundos continuos leyendo\n",
    "PRE_CUT_SECONDS = 1.5\n",
    "\n",
    "INPUT_DIR = \"./audios\"\n",
    "OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d61ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_vad(audio_path):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "\n",
    "    vad_result = vad(audio_path)\n",
    "\n",
    "    segments = []\n",
    "    for segment in vad_result.get_timeline():\n",
    "        segments.append({\n",
    "            \"start\": segment.start,\n",
    "            \"end\": segment.end,\n",
    "            \"duration\": segment.end - segment.start\n",
    "        })\n",
    "\n",
    "    # print('##############################################')\n",
    "    # print(audio_path)\n",
    "    # print(segments)\n",
    "    # print('##############################################')\n",
    "\n",
    "    # Agrupar segmentos cercanos\n",
    "    merged = []\n",
    "    for seg in segments:\n",
    "        if not merged:\n",
    "            merged.append(seg)\n",
    "        else:\n",
    "            last = merged[-1]\n",
    "            if seg[\"start\"] - last[\"end\"] < 0.4:  # tolerancia silencio corto\n",
    "                last[\"end\"] = seg[\"end\"]\n",
    "                last[\"duration\"] = last[\"end\"] - last[\"start\"]\n",
    "            else:\n",
    "                merged.append(seg.copy())\n",
    "\n",
    "\n",
    "    # print('##############################################')\n",
    "    print(audio_path)c:\\Users\\Denisse Orellana\\Desktop\\editar_audio\\test._2.json\n",
    "    print(merged)\n",
    "    # print('##############################################'\n",
    "\n",
    "    # Buscar primer bloque largo\n",
    "    acc = 0.0\n",
    "    for block in merged:\n",
    "        acc += block[\"duration\"]\n",
    "        block[\"duration_acc\"] = acc\n",
    "\n",
    "        # if block[\"duration\"] >= MIN_READING_DURATION:\n",
    "        if block[\"duration_acc\"] >= MIN_READING_DURATION:\n",
    "            cut_time = max(0, block[\"start\"] - PRE_CUT_SECONDS)\n",
    "            cut_audio = audio[int(cut_time * 1000):]\n",
    "            return cut_audio, cut_time\n",
    "\n",
    "    print(\"‚ö†Ô∏è No se detect√≥ lectura continua\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# PROCESAR CARPETA\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.lower().endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(INPUT_DIR, file)\n",
    "    output_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        file.replace(\".wav\", \"_cut.wav\")\n",
    "    )\n",
    "\n",
    "    result = process_audio_vad(input_path)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    cut_audio, cut_time = result\n",
    "    cut_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"‚úî Guardado: {output_path} (corte en {cut_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zip folder with files\n",
    "import os\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "download_dir = './output'\n",
    "zip_path = './output.zip'\n",
    "\n",
    "# Crear un ZIP con todos los archivos\n",
    "shutil.make_archive(zip_path.replace('.zip', ''), 'zip', download_dir)\n",
    "\n",
    "# Descargar el ZIP completo\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61c9ba",
   "metadata": {},
   "source": [
    "_________________________\n",
    "3¬∞ Try: VAD + Construcci√≥n serie temporal de voz\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba773d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "vad = Pipeline.from_pretrained(\n",
    "    \"pyannote/voice-activity-detection\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "INPUT_DIR = \"./audios\"\n",
    "OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8262e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SECONDS = 8.0     # ventana m√°xima de an√°lisis\n",
    "MIN_TOTAL_VOICE = 4.0   # voz acumulada dentro de la ventana\n",
    "PRE_CUT_SECONDS = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reading_start(segments):\n",
    "    \"\"\"\n",
    "    segments: lista de dicts con start, end, duration\n",
    "    return: start_time o None\n",
    "    \"\"\"\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        window_start = seg[\"start\"]\n",
    "        window_end = window_start + WINDOW_SECONDS\n",
    "\n",
    "        total_voice = 0.0\n",
    "\n",
    "        for s in segments[i:]:\n",
    "            if s[\"start\"] > window_end:\n",
    "                break\n",
    "\n",
    "            overlap_start = max(s[\"start\"], window_start)\n",
    "            overlap_end = min(s[\"end\"], window_end)\n",
    "\n",
    "            if overlap_end > overlap_start:\n",
    "                total_voice += overlap_end - overlap_start\n",
    "\n",
    "            if total_voice >= MIN_TOTAL_VOICE:\n",
    "                return window_start\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir serie temporal de voz\n",
    "def build_voice_timeline(segments, resolution=0.1):\n",
    "    timeline = []\n",
    "    t = 0.0\n",
    "    end_time = max(s[\"end\"] for s in segments)\n",
    "\n",
    "    while t < end_time:\n",
    "        is_voice = any(s[\"start\"] <= t < s[\"end\"] for s in segments)\n",
    "        timeline.append((t, 1 if is_voice else 0))\n",
    "        t += resolution\n",
    "\n",
    "    return timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar cambio estructural (sin umbral fijo)\n",
    "import numpy as np\n",
    "\n",
    "def detect_reading_start_adaptive(timeline, resolution=0.1):\n",
    "    values = np.array([v for _, v in timeline])\n",
    "\n",
    "    # promedio m√≥vil acumulado\n",
    "    cumulative_density = np.cumsum(values) / np.arange(1, len(values)+1)\n",
    "\n",
    "    # derivada (cambio)\n",
    "    diff = np.diff(cumulative_density)\n",
    "\n",
    "    # punto de mayor cambio positivo\n",
    "    idx = np.argmax(diff)\n",
    "\n",
    "    return timeline[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29578c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_path):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    vad_result = vad(audio_path)\n",
    "\n",
    "    segments = [{\n",
    "        \"start\": s.start,\n",
    "        \"end\": s.end\n",
    "    } for s in vad_result.get_timeline()]\n",
    "\n",
    "    if not segments:\n",
    "        return None\n",
    "\n",
    "    timeline = build_voice_timeline(segments)\n",
    "    reading_start = detect_reading_start_adaptive(timeline)\n",
    "\n",
    "    # margen din√°mico (proporcional)\n",
    "    pre_cut = max(0.5, reading_start * 0.15)\n",
    "\n",
    "    cut_time = max(0, reading_start - pre_cut)\n",
    "    cut_audio = audio[int(cut_time * 1000):]\n",
    "\n",
    "    return cut_audio, cut_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4494fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# PROCESAR CARPETA\n",
    "# ==========================\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if not file.lower().endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    input_path = os.path.join(INPUT_DIR, file)\n",
    "    output_path = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        file.replace(\".wav\", \"_cut.wav\")\n",
    "    )\n",
    "\n",
    "    result = process_audio(input_path)\n",
    "\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    cut_audio, cut_time = result\n",
    "    cut_audio.export(output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"‚úî Guardado: {output_path} (corte en {cut_time:.2f}s)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
